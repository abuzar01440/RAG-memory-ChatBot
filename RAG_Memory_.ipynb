{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V0Ayy7Yd5F54",
        "outputId": "97bd6279-bbfc-4967-a550-b34599d1c79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.9)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.7-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m558.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain_google_genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain_google_genai-2.1.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "c49fddfb13184541bf51ae6d145294a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinecone\n",
            "  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.7.9)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.14.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.4.0)\n",
            "Requirement already satisfied: packaging<25.0,>=24.2 in /usr/local/lib/python3.11/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
            "Downloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-assistant, pinecone\n",
            "Successfully installed pinecone-7.3.0 pinecone-plugin-assistant-1.7.0 pinecone-plugin-interface-0.0.7\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.68)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.9)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain langchain_google_genai\n",
        "! pip install pinecone\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain_pinecone\n",
        "! pip install langchain_huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mXfEjOdHDAc",
        "outputId": "7f9886a5-d17c-472e-d769-49940b96dd70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.2.8-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain_pinecone) (0.3.68)\n",
            "Requirement already satisfied: pinecone<8.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (7.3.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain_pinecone) (2.0.2)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain_pinecone)\n",
            "  Downloading langchain_tests-0.3.20-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langchain-openai>=0.3.11 (from langchain_pinecone)\n",
            "  Downloading langchain_openai-0.3.27-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.4.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (4.14.1)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (2.11.7)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.11->langchain_pinecone) (1.93.3)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.11->langchain_pinecone) (0.9.0)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (8.3.5)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.28.1)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting pytest-benchmark (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_benchmark-5.1.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pytest-codspeed (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_codspeed-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.3 kB)\n",
            "Collecting pytest-recording (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_recording-0.13.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting vcrpy>=7.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading vcrpy-7.0.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (2025.7.9)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.7.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (2.4.0)\n",
            "Requirement already satisfied: aiohttp>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (3.11.15)\n",
            "Collecting aiohttp-retry<3.0.0,>=2.9.1 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.23.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.4.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.17.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai>=0.3.11->langchain_pinecone) (2024.11.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from vcrpy>=7.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.17.2)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (9.0.0)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.17.1)\n",
            "Requirement already satisfied: rich>=13.8.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (13.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.17.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.1.2)\n",
            "Downloading langchain_pinecone-0.2.8-py3-none-any.whl (22 kB)\n",
            "Downloading langchain_openai-0.3.27-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m716.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_tests-0.3.20-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m982.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vcrpy-7.0.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_benchmark-5.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_codspeed-4.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.1/221.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_recording-0.13.4-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: vcrpy, syrupy, pytest-socket, pytest-benchmark, pytest-asyncio, pytest-recording, pytest-codspeed, aiohttp-retry, langchain-tests, langchain-openai, langchain_pinecone\n",
            "Successfully installed aiohttp-retry-2.9.1 langchain-openai-0.3.27 langchain-tests-0.3.20 langchain_pinecone-0.2.8 pytest-asyncio-0.26.0 pytest-benchmark-5.1.0 pytest-codspeed-4.0.0 pytest-recording-0.13.4 pytest-socket-0.7.0 syrupy-4.9.1 vcrpy-7.0.0\n",
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-0.3.0-py3-none-any.whl.metadata (996 bytes)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.65 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.3.68)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain_huggingface) (0.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain_huggingface) (1.1.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.4.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.11.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.30.2->langchain_huggingface) (2025.7.9)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.65->langchain_huggingface) (1.3.1)\n",
            "Downloading langchain_huggingface-0.3.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: langchain_huggingface\n",
            "Successfully installed langchain_huggingface-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_pinecone import PineconeVectorStore # Import PineconeVectorStore\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "import pinecone\n",
        "import getpass\n",
        "# Import the Pinecone library\n",
        "from pinecone import Pinecone\n",
        "from langchain_huggingface import HuggingFaceEmbeddings # new update in langchain\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from time import time"
      ],
      "metadata": {
        "id": "a7pMJLC75OqZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GPbpn9355io5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpea3UEX6vAM",
        "outputId": "080961e9-4900-44e3-cbb0-9d330086cb70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google AI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"PINECONE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")"
      ],
      "metadata": {
        "id": "zlCzk2NZIDnM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY=\"Write/Paste your key here\""
      ],
      "metadata": {
        "id": "WmUo1N73U_jd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3AKXQ0uXwE1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make a dir pdf of papers\n",
        "\n",
        "if not os.path.exists(\"papers\"):\n",
        "    os.makedirs(\"papers\")"
      ],
      "metadata": {
        "id": "-73jV9S57o1E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory =\"/content/papers\""
      ],
      "metadata": {
        "id": "rxadHeil8bHy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "papers=os.listdir(directory)\n",
        "papers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QopD01EE77bO",
        "outputId": "d3beac37-90a8-4c92-9180-7a064a2c4c36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Weakly_Supervised_Deep_Learning_for_the_Detection_of_Domain_Generation_Algorithms.pdf',\n",
              " 'deep fake weak supervise learning - Consensus.pdf',\n",
              " 'journal.pone.0326565.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"total number of PDFs:\", len(papers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVbCI3DI8XbL",
        "outputId": "264e5b4b-3567-488b-8e3c-dd7cd9e3ba33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of PDFs: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INDEX_NAME = \"rag-pdf-index\"  # Pinecone index name"
      ],
      "metadata": {
        "id": "mTkUXVxS9WGp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"  # Better embedding model"
      ],
      "metadata": {
        "id": "l4UWgGiJ9va9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_pinecone():\n",
        "    \"\"\"Initialize Pinecone and create index if needed\"\"\"\n",
        "    try:\n",
        "        # Initialize Pinecone client with explicit API key\n",
        "        pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "\n",
        "        # List existing indexes\n",
        "        existing_indexes = pc.list_indexes().names()\n",
        "        print(f\"Existing indexes: {existing_indexes}\")\n",
        "\n",
        "        # Create index if it doesn't exist\n",
        "        if INDEX_NAME not in existing_indexes:\n",
        "            print(f\"Creating new index: {INDEX_NAME}\")\n",
        "            pc.create_index(\n",
        "                name=INDEX_NAME,\n",
        "                dimension=768,  # Match embedding dimension for all-mpnet-base-v2\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud=\"aws\",\n",
        "                    region=\"us-east-1\"\n",
        "                )\n",
        "            )\n",
        "            print(f\"Index {INDEX_NAME} created successfully\")\n",
        "            # Wait for index to be ready\n",
        "            time.sleep(10)\n",
        "        else:\n",
        "            print(f\"Using existing index: {INDEX_NAME}\")\n",
        "\n",
        "        return pc.Index(INDEX_NAME)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Pinecone: {e}\")\n",
        "        print(\"Please check your Pinecone API key and try again.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "No_LOD2_918f"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8QLJccWBY9i",
        "outputId": "2e02ddaf-ed66-4e8a-f81e-2769ac75f1ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_filename(path):\n",
        "    \"\"\"Extract clean filename from path\"\"\"\n",
        "    filename = os.path.basename(path)\n",
        "    return re.sub(r'[^a-zA-Z0-9_.-]', ' ', filename)"
      ],
      "metadata": {
        "id": "lBPTJ6i-OYpq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_documents():\n",
        "    \"\"\"Load and process PDF documents\"\"\"\n",
        "    # Check if papers directory has files\n",
        "    papers = os.listdir(directory)\n",
        "    pdf_files = [f for f in papers if f.lower().endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in {directory}\")\n",
        "        print(\"Please upload PDF files to the papers directory.\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF files: {pdf_files}\")\n",
        "\n",
        "    # Load documents\n",
        "    loader = PyPDFDirectoryLoader(directory, recursive=True)\n",
        "    raw_documents = loader.load()\n",
        "    print(f\"Loaded {len(raw_documents)} pages from PDFs\")\n",
        "\n",
        "    # Clean metadata\n",
        "    documents = []\n",
        "    for doc in raw_documents:\n",
        "        clean_source = clean_filename(doc.metadata['source'])\n",
        "        doc.metadata.update({\n",
        "            'source': clean_source,\n",
        "            'filename': clean_source\n",
        "        })\n",
        "        documents.append(doc)\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1200,\n",
        "        chunk_overlap=150,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Split into {len(chunks)} text chunks\")\n",
        "\n",
        "    return documents, chunks"
      ],
      "metadata": {
        "id": "VYLZXV9q_h4F"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(chunks):\n",
        "    \"\"\"Create vector store with embeddings\"\"\"\n",
        "    # Initialize embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=EMBEDDING_MODEL,\n",
        "        model_kwargs={'device': 'cpu'},\n",
        "        encode_kwargs={'normalize_embeddings': True}\n",
        "    )\n",
        "    print(\"Embeddings model loaded\")\n",
        "\n",
        "    # Initialize Pinecone\n",
        "    pinecone_index = initialize_pinecone()\n",
        "    if pinecone_index is None:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Create vector store\n",
        "        vector_store = PineconeVectorStore.from_documents(\n",
        "            chunks,\n",
        "            embeddings,\n",
        "            index_name=INDEX_NAME,\n",
        "            pinecone_api_key=os.environ[\"PINECONE_API_KEY\"]\n",
        "        )\n",
        "        print(\"Documents embedded and stored in Pinecone successfully\")\n",
        "        return vector_store\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating vector store: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "yhhcdXpzXM5X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_qa_chain(vector_store):\n",
        "    \"\"\"Setup the conversational QA chain\"\"\"\n",
        "    # Create retriever\n",
        "    retriever = vector_store.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\n",
        "            \"k\": 5,\n",
        "            \"score_threshold\": 0.7,\n",
        "            \"filter\": {\"source\": {\"$exists\": True}}\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Initialize Gemini LLM\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-flash\",\n",
        "        temperature=0.6,\n",
        "        max_output_tokens=1500,\n",
        "        top_k=40,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "    # Create conversation memory\n",
        "    memory = ConversationBufferWindowMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        k=3,\n",
        "        return_messages=True,\n",
        "        output_key=\"answer\"\n",
        "    )\n",
        "\n",
        "    # Create conversational RAG chain\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        memory=memory,\n",
        "        return_source_documents=True,\n",
        "        get_chat_history=lambda h: h\n",
        "    )\n",
        "\n",
        "    return qa_chain"
      ],
      "metadata": {
        "id": "MdjebvdcXUNK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to set up the RAG system\"\"\"\n",
        "    print(\"Setting up RAG system...\")\n",
        "\n",
        "    # Step 1: Load and process documents\n",
        "    documents, chunks = load_and_process_documents()\n",
        "    if chunks is None:\n",
        "        return\n",
        "\n",
        "    # Step 2: Create vector store\n",
        "    vector_store = create_vector_store(chunks)\n",
        "    if vector_store is None:\n",
        "        return\n",
        "\n",
        "    # Step 3: Setup QA chain\n",
        "    qa_chain = setup_qa_chain(vector_store)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"RAG system ready! You can now ask questions about your documents.\")\n",
        "    print(\"Type 'bye', 'exit', or 'quit' to end the conversation.\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Interactive chat loop\n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"\\nYou: \")\n",
        "\n",
        "            if query.lower().strip() in [\"bye\", \"exit\", \"quit\", \"\"]:\n",
        "                print(\"Assistant: Goodbye! Have a great day!\")\n",
        "                break\n",
        "\n",
        "            # Process the query\n",
        "            print(\"Assistant: Thinking...\")\n",
        "            result = qa_chain({\"question\": query})\n",
        "\n",
        "            # Display response\n",
        "            print(f\"\\nAssistant: {result['answer']}\")\n",
        "\n",
        "            # Show sources if requested\n",
        "            if any(word in query.lower() for word in [\"source\", \"reference\", \"cite\", \"paper\"]):\n",
        "                print(\"\\nSources:\")\n",
        "                for i, doc in enumerate(result[\"source_documents\"][:3]):\n",
        "                    source = doc.metadata.get('source', 'Unknown')\n",
        "                    page = doc.metadata.get('page', 'N/A')\n",
        "                    print(f\"{i+1}. {os.path.basename(source)} (Page {page})\")\n",
        "                    print(f\"   Excerpt: {doc.page_content[:150]}...\\n\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nAssistant: Goodbye! Have a great day!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError: {e}\")\n",
        "            print(\"Please try again with a different question.\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBQd2C2HXY20",
        "outputId": "dfac6924-6dcc-4cad-ca36-445e6d0079c2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up RAG system...\n",
            "Found 3 PDF files: ['Weakly_Supervised_Deep_Learning_for_the_Detection_of_Domain_Generation_Algorithms.pdf', 'deep fake weak supervise learning - Consensus.pdf', 'journal.pone.0326565.pdf']\n",
            "Loaded 58 pages from PDFs\n",
            "Split into 195 text chunks\n",
            "Embeddings model loaded\n",
            "Existing indexes: ['rag-pdf-index']\n",
            "Using existing index: rag-pdf-index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-19-2749410998.py:23: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents embedded and stored in Pinecone successfully\n",
            "\n",
            "==================================================\n",
            "RAG system ready! You can now ask questions about your documents.\n",
            "Type 'bye', 'exit', or 'quit' to end the conversation.\n",
            "==================================================\n",
            "\n",
            "You: \"\"\"     Based on all the research papers you have access to, write a comprehensive literature review paragraph      in IEEE citation style. The paragraph should:          1. Summarize the main contributions of each paper     2. Highlight the pros and cons of different approaches     3. Identify research gaps and limitations     4. Show the progression of ideas in the field     5. Be written in academic style suitable for a research paper          Please provide a well-structured literature review paragraph that synthesizes      the findings from all available papers.     \"\"\"\n",
            "Assistant: Thinking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-23-4217766465.py:34: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = qa_chain({\"question\": query})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assistant: Recent research has explored weakly supervised deep learning for various tasks.  Varma et al. [1]  pioneered the use of deep reinforcement learning for anomaly detection in business processes, effectively leveraging limited labeled data, but the specifics of its generalizability remain unclear.  Elaziz et al. [2] further advanced this approach, demonstrating improved performance compared to competing methods by exploring unlabeled data.  Karamanolakis et al. [3] introduced ASTRA, a self-training framework that enhanced text classification by effectively utilizing all available data, showcasing the potential of weak supervision for improving deep neural network performance.  Zhao et al. [4] applied weakly supervised deep embedding to product review sentiment analysis, effectively classifying review sentences using ratings as weak supervision signals.  While these studies demonstrate the effectiveness of weakly supervised deep learning across diverse applications, limitations exist.  The reliance on weak supervision signals can introduce noise and bias [2, 4], and the generalizability of these models across different datasets and tasks requires further investigation [1, 3].  Future research should focus on developing more robust and generalizable weakly supervised learning methods that mitigate the effects of noisy labels and improve the efficiency of data utilization.  Furthermore, exploring the integration of different weak supervision strategies and developing techniques for evaluating the quality of weak supervision signals are crucial for advancing the field.\n",
            "\n",
            "[1] P. Varma et al., “Deep reinforcement learning for data-efficient weakly supervised business process anomaly detection,” *Proceedings of the VLDB Endowment*, vol. 7, 2019.\n",
            "[2] E. A. Elaziz et al., “Journal of Big Data,” 2023.\n",
            "[3] G. Karamanolakis et al., “Self-Training with Weak Supervision,” *ArXiv*, 2021.\n",
            "[4] W. Zhao et al., “Weakly-Supervised Deep Embedding for Product Review Sentiment Analysis,” *IEEE Transactions on Knowledge and Data Engineering*, 2018.\n",
            "\n",
            "Sources:\n",
            "1. Weakly_Supervised_Deep_Learning_for_the_Detection_of_Domain_Generation_Algorithms.pdf (Page 0.0)\n",
            "   Excerpt: 51542\n",
            "2169-3536 \n",
            " 2019 IEEE. Translations and content mining are permitted for academic research only.\n",
            "Personal use is also permitted, but republicati...\n",
            "\n",
            "2. deep fake weak supervise learning - Consensus.pdf (Page 11.0)\n",
            "   Excerpt: 2018· 152citations · P. Varma et al.\n",
            "Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases   \n",
            "23Deep reinforcement ...\n",
            "\n",
            "3. journal.pone.0326565.pdf (Page 16.0)\n",
            "   Excerpt: • Accuracy: The accuracy is a fundamental metric that measures the proportion of correct\n",
            "predictions made by the model out of the total predictions. H...\n",
            "\n",
            "\n",
            "You: what accuracy evuation metrics used in these studies ? what are benchmark achieved \n",
            "Assistant: Thinking...\n",
            "\n",
            "Assistant: The provided text mentions AUC (Area Under the Curve) scores as an evaluation metric in one study,  reporting scores ranging from 44.45% to 72.69% on prospective data.  However, it does not offer benchmark accuracies for comparison across all cited studies.  Several papers mention achieving \"convincing performance\" or outperforming competing approaches, but lack specific numerical benchmark accuracies.\n",
            "\n",
            "You: summarize this paper \"Single-layer KAN for deepfake classification: Balancing efficiency and performance in resource constrained environments\"\n",
            "Assistant: Thinking...\n",
            "\n",
            "Assistant: The research paper \"Single-layer KAN for deepfake classification: Balancing efficiency and performance in resource-constrained environments\" introduces a single-layer Kolmogorov-Arnold Network (KAN) for deepfake detection.  The model uses a SiLU activation function and a spline-based transformation, aiming for efficiency while maintaining performance.  The study compares the KAN model to benchmark Convolutional Neural Networks (CNNs), highlighting its advantages in terms of memory usage and computational efficiency, particularly relevant for resource-constrained environments.  The KAN model was trained on the Celeb-DF and FaceForensics++ datasets, and its performance is evaluated and compared to other methods, including DefakeHop, which achieved 90.56% accuracy with 42.8K parameters.  The paper also discusses the use of the Augur tool for assessing the computational requirements of CNNs on edge devices.\n",
            "\n",
            "Sources:\n",
            "1. journal.pone.0326565.pdf (Page 19.0)\n",
            "   Excerpt: ID: pone.0326565 — 2025/7/4 — page 20 — #20\n",
            "PLOS One Single-layer KAN for deepfake classification\n",
            "Fig 9.Memory and accuracy of benchmark CNN vs propos...\n",
            "\n",
            "2. journal.pone.0326565.pdf (Page 13.0)\n",
            "   Excerpt: ID: pone.0326565 — 2025/7/4 — page 14 — #14\n",
            "PLOS One Single-layer KAN for deepfake classification\n",
            "This is the core equation of the KAN. Each input fea...\n",
            "\n",
            "3. journal.pone.0326565.pdf (Page 15.0)\n",
            "   Excerpt: ID: pone.0326565 — 2025/7/4 — page 16 — #16\n",
            "PLOS One Single-layer KAN for deepfake classification\n",
            "where H and W were the dimensions of the output feat...\n",
            "\n",
            "\n",
            "You: nice\n",
            "Assistant: Thinking...\n",
            "\n",
            "Assistant: You are correct.  There is no question to answer.\n",
            "\n",
            "You: bye\n",
            "Assistant: Goodbye! Have a great day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assistant: Recent research has explored weakly supervised deep learning for various tasks.  Varma et al. [1]  pioneered the use of deep reinforcement learning for anomaly detection in business processes, effectively leveraging limited labeled data, but the specifics of its generalizability remain unclear.  Elaziz et al. [2] further advanced this approach, demonstrating improved performance compared to competing methods by exploring unlabeled data.  Karamanolakis et al. [3] introduced ASTRA, a self-training framework that enhanced text classification by effectively utilizing all available data, showcasing the potential of weak supervision for improving deep neural network performance.  Zhao et al. [4] applied weakly supervised deep embedding to product review sentiment analysis, effectively classifying review sentences using ratings as weak supervision signals.  While these studies demonstrate the effectiveness of weakly supervised deep learning across diverse applications, limitations exist.  The reliance on weak supervision signals can introduce noise and bias [2, 4], and the generalizability of these models across different datasets and tasks requires further investigation [1, 3].  Future research should focus on developing more robust and generalizable weakly supervised learning methods that mitigate the effects of noisy labels and improve the efficiency of data utilization.  Furthermore, exploring the integration of different weak supervision strategies and developing techniques for evaluating the quality of weak supervision signals are crucial for advancing the field.\n",
        "\n",
        "[1] P. Varma et al., “Deep reinforcement learning for data-efficient weakly supervised business process anomaly detection,” *Proceedings of the VLDB Endowment*, vol. 7, 2019.\n",
        "[2] E. A. Elaziz et al., “Journal of Big Data,” 2023.\n",
        "[3] G. Karamanolakis et al., “Self-Training with Weak Supervision,” *ArXiv*, 2021.\n",
        "[4] W. Zhao et al., “Weakly-Supervised Deep Embedding for Product Review Sentiment Analysis,” *IEEE Transactions on Knowledge and Data Engineering*, 2018."
      ],
      "metadata": {
        "id": "izvd2SVuZhcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assistant: The provided text mentions AUC (Area Under the Curve) scores as an evaluation metric in one study,  reporting scores ranging from 44.45% to 72.69% on prospective data.  However, it does not offer benchmark accuracies for comparison across all cited studies.  Several papers mention achieving \"convincing performance\" or outperforming competing approaches, but lack specific numerical benchmark accuracies.\n"
      ],
      "metadata": {
        "id": "5jMNxqr8ZoUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assistant: The research paper \"Single-layer KAN for deepfake classification: Balancing efficiency and performance in resource-constrained environments\" introduces a single-layer Kolmogorov-Arnold Network (KAN) for deepfake detection.  The model uses a SiLU activation function and a spline-based transformation, aiming for efficiency while maintaining performance.  The study compares the KAN model to benchmark Convolutional Neural Networks (CNNs), highlighting its advantages in terms of memory usage and computational efficiency, particularly relevant for resource-constrained environments.  The KAN model was trained on the Celeb-DF and FaceForensics++ datasets, and its performance is evaluated and compared to other methods, including DefakeHop, which achieved 90.56% accuracy with 42.8K parameters.  The paper also discusses the use of the Augur tool for assessing the computational requirements of CNNs on edge devices.\n"
      ],
      "metadata": {
        "id": "VSIS-PX5aKOb"
      }
    }
  ]
}